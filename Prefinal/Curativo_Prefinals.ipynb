{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a1f211-970b-42ac-910e-985d777822a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting liac-arff\n",
      "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: liac-arff\n",
      "  Building wheel for liac-arff (setup.py): started\n",
      "  Building wheel for liac-arff (setup.py): finished with status 'done'\n",
      "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11727 sha256=65f123fe04976c4418f4ae9bfa1c4108ae7ccc271dd9ef713c67c3cb1fa10a18\n",
      "  Stored in directory: c:\\users\\pc user\\appdata\\local\\pip\\cache\\wheels\\00\\23\\31\\5e562fce1f95aabe57f2a7320d07433ba1cd152bcde2f6a002\n",
      "Successfully built liac-arff\n",
      "Installing collected packages: liac-arff\n",
      "Successfully installed liac-arff-2.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install liac-arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7758ffb4-7f26-48ec-8e09-edc6b39223d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the data from each file\n",
    "data_list = []\n",
    "\n",
    "# List of ARFF file names\n",
    "arff_files = ['2017 Q1.arff',\n",
    "'2017 Q2.arff',\n",
    "'2017 Q3.arff',\n",
    "'2017 Q4.arff',\n",
    "'2017.arff',\n",
    "'2018 Q1.arff',\n",
    "'2018 Q2.arff',\n",
    "'2018 Q3.arff',\n",
    "'2018 Q4.arff',\n",
    "'2018.arff',\n",
    "'2019 Q1.arff',\n",
    "'2019 Q2.arff',\n",
    "'2019 Q3.arff',\n",
    "'2019 Q4.arff',\n",
    "'2019.arff',\n",
    "'2020 Q1.arff',\n",
    "'2020 Q2.arff',\n",
    "'2020 Q3.arff',\n",
    "'2020 Q4.arff',\n",
    "'2020.arff',\n",
    "'2021 Q1.arff'] \n",
    "\n",
    "# Read data from each ARFF file\n",
    "for file_name in arff_files:\n",
    "    with open(f'V4 data/{file_name}', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        data_start = False\n",
    "        current_data = []\n",
    "\n",
    "        for line in lines:\n",
    "            if data_start:\n",
    "                # Assuming data lines start with '@data' and end with the end of the file\n",
    "                current_data.append(line.strip().split(','))\n",
    "            elif '@data' in line:\n",
    "                data_start = True\n",
    "\n",
    "        data_list.append(current_data)\n",
    "\n",
    "# Combine data from all files into a single matrix\n",
    "combined_matrix = [row for file_data in data_list for row in file_data]\n",
    "\n",
    "\n",
    "def convert_to_float(value):\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            return float(value) if value.replace('.', '', 1).isdigit() or (value[1:].replace('.', '', 1).isdigit() and value[0] == '-') else None\n",
    "        elif isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "        else:\n",
    "            return None\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "combined_matrix = [[convert_to_float(entry) for entry in row] for row in combined_matrix]\n",
    "X = np.array(combined_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6df9a12d-8000-42db-adc6-d12d11000499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.0, None, None, ..., None, None, 1.0],\n",
       "       [22.0, None, 0.0, ..., None, 1.0, 1.0],\n",
       "       [27.0, None, 0.01, ..., None, None, 1.0],\n",
       "       ...,\n",
       "       [427.0, None, None, ..., None, None, 6.0],\n",
       "       [432.0, None, 0.0, ..., 0.0, 0.0, 6.0],\n",
       "       [438.0, None, None, ..., None, None, 6.0]], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99893281-2cd7-4aa5-b40d-ba4dd904b8fd",
   "metadata": {},
   "source": [
    "### PCA from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4219cd52-d1e2-4263-bc27-25a5fa003e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import eig\n",
    "\n",
    "class Curativo_PCA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components   \n",
    "        \n",
    "    def fit(self, X):\n",
    "        ### Data Standardization\n",
    "        # getting the mean\n",
    "        mean = sum(X) / len(X)\n",
    "        # getting std\n",
    "        std = (sum((trav - mean) ** 2 for trav in X) / len(X)) ** 0.5\n",
    "        self.X_std = (X - mean) / std\n",
    "\n",
    "        ### Covariance Matrix\n",
    "        # Perform matrix multiplication (@) on self.X_std and its transpose\n",
    "        covariance_matrix = (self.X_std.T @ self.X_std)/(self.X_std.shape[0]-1)\n",
    "\n",
    "        ### Eigenvalues and Eigenvectors\n",
    "        # Eigendecomposition of covariance matrix\n",
    "        eigenvalues, eigenvectors = eig(covariance_matrix) \n",
    "        # Adjusting the eigenvectors (loadings) that are largest in absolute value to be positive\n",
    "        max_abs_idx = np.argmax(np.abs(eigenvectors), axis=0)\n",
    "        signs = np.sign(eigenvectors[max_abs_idx, range(eigenvectors.shape[0])])\n",
    "        eigenvectors = eigenvectors * signs[np.newaxis, :]\n",
    "        eigenvectors = eigenvectors.T\n",
    "        \n",
    "        ### Rearrange in descending order\n",
    "        # Create eigenpairs from eigenvalues and eigenvectors tuples\n",
    "        eigenpairs = [(np.abs(eigenvalues[i]), eigenvectors[i, :]) for i in range(len(eigenvalues))]\n",
    "        # Sort eigenpairs from the highest to the lowest based on eigenvalues magnitude\n",
    "        eigenpairs.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Extract sorted eigenvalues and eigenvectors\n",
    "        eigenvalues_sorted, eigenvectors_sorted = zip(*eigenpairs)\n",
    "        # Convert the results to numpy arrays\n",
    "        eigenvalues_sorted = np.array(eigenvalues_sorted)\n",
    "        eigenvectors_sorted = np.array(eigenvectors_sorted)\n",
    "        \n",
    "        ### Principal Components\n",
    "        # self.n_components == k\n",
    "        self.components = eigenvectors_sorted[:self.n_components, :]\n",
    "\n",
    "        ### Explained Variance\n",
    "        eigenvalues_total = sum(eigenvalues)\n",
    "        self.explained_variance_ratio = [(i / eigenvalues_total) for i in eigenvalues_sorted[:self.n_components]]\n",
    "        self.cum_explained_variance = np.cumsum(self.explained_variance_ratio)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        ### Data Projection\n",
    "        X = X.copy()\n",
    "        X_projection = self.X_std.dot(self.components.T)\n",
    "\n",
    "        return X_projection\n",
    "\n",
    "\n",
    "\n",
    "# my_pca = Curativo_PCA(n_components = 2).fit(X)\n",
    "\n",
    "# print('Components:\\n', my_pca.components)\n",
    "# print('Explained variance ratio from scratch:\\n', my_pca.explained_variance_ratio)\n",
    "# print('Cumulative explained variance from scratch:\\n', my_pca.cum_explained_variance)\n",
    "\n",
    "# X_proj = my_pca.transform(X)\n",
    "# print('Transformed data shape from scratch:', X_proj.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d485f616-7832-4e95-b58e-37c503190e97",
   "metadata": {},
   "source": [
    "### Testing My PCA (Using the Given Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bba43f9c-7e62-4406-a9a8-5008382855b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc user\\AppData\\Local\\Temp\\ipykernel_9104\\2423504760.py:13: RuntimeWarning: invalid value encountered in divide\n",
      "  self.X_std = (X - mean) / std\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack((numerical_data, second_feature))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Perform PCA with my own pca\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m my_pca \u001b[38;5;241m=\u001b[39m \u001b[43mCurativo_PCA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComponents:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, my_pca\u001b[38;5;241m.\u001b[39mcomponents)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExplained variance ratio:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, my_pca\u001b[38;5;241m.\u001b[39mexplained_variance_ratio)\n",
      "Cell \u001b[1;32mIn[67], line 21\u001b[0m, in \u001b[0;36mCurativo_PCA.fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     17\u001b[0m covariance_matrix \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_std\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_std)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_std\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m### Eigenvalues and Eigenvectors\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Eigendecomposition of covariance matrix\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m eigenvalues, eigenvectors \u001b[38;5;241m=\u001b[39m \u001b[43meig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariance_matrix\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Adjusting the eigenvectors (loadings) that are largest in absolute value to be positive\u001b[39;00m\n\u001b[0;32m     23\u001b[0m max_abs_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39mabs(eigenvectors), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meig\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\numpy\\linalg\\linalg.py:1298\u001b[0m, in \u001b[0;36meig\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   1296\u001b[0m _assert_stacked_2d(a)\n\u001b[0;32m   1297\u001b[0m _assert_stacked_square(a)\n\u001b[1;32m-> 1298\u001b[0m \u001b[43m_assert_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1299\u001b[0m t, result_t \u001b[38;5;241m=\u001b[39m _commonType(a)\n\u001b[0;32m   1301\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(\n\u001b[0;32m   1302\u001b[0m     _raise_linalgerror_eigenvalues_nonconvergence)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\numpy\\linalg\\linalg.py:195\u001b[0m, in \u001b[0;36m_assert_finite\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isfinite(a)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m--> 195\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArray must not contain infs or NaNs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "# Extract numerical features (excluding the second column)\n",
    "numerical_data = X[:, 0].astype(float)\n",
    "\n",
    "# Create a synthetic second feature with constant values\n",
    "second_feature = np.ones_like(numerical_data)\n",
    "\n",
    "# Stack the features horizontally\n",
    "combined_features = np.column_stack((numerical_data, second_feature))\n",
    "\n",
    "# Perform PCA with my own pca\n",
    "my_pca = Curativo_PCA(n_components = 2).fit(X_std)\n",
    "\n",
    "print('Components:\\n', my_pca.components)\n",
    "print('Explained variance ratio:\\n', my_pca.explained_variance_ratio)\n",
    "\n",
    "cum_explained_variance = np.cumsum(my_pca.explained_variance_ratio)\n",
    "print('Cumulative explained variance:\\n', cum_explained_variance)\n",
    "\n",
    "X_pca = my_pca.transform(X_std) # Apply dimensionality reduction to X.\n",
    "print('Transformed data shape:', X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c88cfc-66f8-48e2-bb0d-5ad89a0479d0",
   "metadata": {},
   "source": [
    "### PCA from Scikit-Learn (Using the Given Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3e232c52-7a15-410d-83fb-db15f7d9ae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:\n",
      " [[1. 0.]\n",
      " [0. 1.]]\n",
      "Explained variance ratio:\n",
      " [1. 0.]\n",
      "Cumulative explained variance:\n",
      " [1. 1.]\n",
      "Transformed data shape: (9450, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Extract numerical features (excluding the second column)\n",
    "numerical_data = X[:, 0].astype(float)\n",
    "\n",
    "# Create a synthetic second feature with constant values\n",
    "second_feature = np.ones_like(numerical_data)\n",
    "\n",
    "# Stack the features horizontally\n",
    "combined_features = np.column_stack((numerical_data, second_feature))\n",
    "\n",
    "X_std = StandardScaler().fit_transform(combined_features)\n",
    "\n",
    "# Perform PCA with the library\n",
    "pca = PCA(n_components = 2).fit(X_std)\n",
    "\n",
    "print('Components:\\n', pca.components_)\n",
    "print('Explained variance ratio:\\n', pca.explained_variance_ratio_)\n",
    "\n",
    "cum_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print('Cumulative explained variance:\\n', cum_explained_variance)\n",
    "\n",
    "X_pca = pca.transform(X_std) # Apply dimensionality reduction to X.\n",
    "print('Transformed data shape:', X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc8439-38ff-4d03-8826-dffff991dcf4",
   "metadata": {},
   "source": [
    "### Testing My PCA (Using Iris Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8dfd86f7-2b32-4791-82ab-01536f9c51cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:\n",
      " [[ 0.52106591 -0.26934744  0.5804131   0.56485654]\n",
      " [ 0.37741762  0.92329566  0.02449161  0.06694199]]\n",
      "Explained variance ratio from scratch:\n",
      " [0.7296244541329985, 0.2285076178670178]\n",
      "Cumulative explained variance from scratch:\n",
      " [0.72962445 0.95813207]\n",
      "Transformed data shape from scratch: (150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "my_pca = Curativo_PCA(n_components = 2).fit(X)\n",
    "\n",
    "print('Components:\\n', my_pca.components)\n",
    "print('Explained variance ratio from scratch:\\n', my_pca.explained_variance_ratio)\n",
    "print('Cumulative explained variance from scratch:\\n', my_pca.cum_explained_variance)\n",
    "\n",
    "X_proj = my_pca.transform(X)\n",
    "print('Transformed data shape from scratch:', X_proj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05588479-66b8-4d76-87e8-db1093680b9a",
   "metadata": {},
   "source": [
    "### PCA from Scikit-Learn (Using Iris Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6c3c7fdf-86d1-4869-bf14-4de14d6f7ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:\n",
      " [[ 0.52106591 -0.26934744  0.5804131   0.56485654]\n",
      " [ 0.37741762  0.92329566  0.02449161  0.06694199]]\n",
      "Explained variance ratio:\n",
      " [0.72962445 0.22850762]\n",
      "Cumulative explained variance:\n",
      " [0.72962445 0.95813207]\n",
      "Transformed data shape: (150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2).fit(X_std)\n",
    "\n",
    "print('Components:\\n', pca.components_)\n",
    "print('Explained variance ratio:\\n', pca.explained_variance_ratio_)\n",
    "\n",
    "cum_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print('Cumulative explained variance:\\n', cum_explained_variance)\n",
    "\n",
    "X_pca = pca.transform(X_std) # Apply dimensionality reduction to X.\n",
    "print('Transformed data shape:', X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417d636-e962-4738-b171-fc3800a2b9c8",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "My PCA does not work using the given dataset because of NaN values due to improper preparation of the dataset.\n",
    "\n",
    "The discrepancy between the Curativo_ PCA code and scikit-learn's PCA library likely stems from the way each approach handles specific scenarios. In my manual implementation, there is an issue with the data standardization process, as evidenced by the warning \"invalid value encountered in divide.\" This suggests that during the standardization, there might be a division by zero or another undefined operation, resulting in NaN or inf values in the data.\n",
    "\n",
    "On the other hand, scikit-learn's PCA implementation might have incorporated additional checks or mechanisms to handle edge cases more gracefully. It could be robustly managing situations where features have constant values or negligible variation, preventing numerical instability during the standardization process. The library likely includes safeguards to avoid division by zero and ensure a smooth computation. However, using another dataset (the Iris dataset), my PCA works and provides the same results generated by the Scikit-Learn's PCA for several reasons:\n",
    "\n",
    "Firstly, both implementations follow a standardized procedure, starting with the standardization of data by centering and scaling. Subsequently, they calculate the covariance matrix from the standardized data and perform eigendecomposition to obtain eigenvalues and eigenvectors. Both implementations sort these eigenvalues and eigenvectors in descending order based on eigenvalue magnitude, and adjust the signs of the eigenvectors for consistency. The selection of a specified number of principal components (controlled by n_components) is a commonality, as is the calculation of explained variance ratios and cumulative explained variance. Finally, both implementations transform the original data using the selected principal components. Overall, these shared steps contribute to the expectation that my custom implementation produces results akin to scikit-learn's PCA. However, it must be noted that while both implementations follow the same algorithmic principles, slight differences may arise due to variations in numerical precision and specific implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a896e7f3-0325-4793-99ca-c29fe01f8c74",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition from Scratch (Using a Random Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "64d073cf-a37f-4998-8d62-6b34923aba2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmas [2.57223046 1.12178285 0.38969376]\n",
      "u: data points in new coordinate system [[ 0.31500548 -0.03347854  0.06858326]\n",
      " [ 0.45305765  0.08238457 -0.55510515]\n",
      " [ 0.32832475  0.28893178 -0.44266707]\n",
      " [ 0.21235219  0.02664493  0.07840391]\n",
      " [ 0.39056255 -0.42227855 -0.15691987]\n",
      " [ 0.26469694  0.53402042  0.36459487]\n",
      " [ 0.43199514 -0.55382733  0.4186888 ]\n",
      " [ 0.36523464  0.37105087  0.39012782]]\n",
      "v transpose: change of basis matrix [[ 0.50399956  0.4444461   0.74057553]\n",
      " [ 0.81511492 -0.52829626 -0.23767778]\n",
      " [ 0.28560832  0.72344366 -0.62853573]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from random import normalvariate\n",
    "from math import sqrt\n",
    "\n",
    "def generate_random_unit_vector(size):\n",
    "    unnormalized_vector = [normalvariate(0, 1) for _ in range(size)]\n",
    "    vector_norm = sqrt(sum(v * v for v in unnormalized_vector))\n",
    "    return [v / vector_norm for v in unnormalized_vector]\n",
    "\n",
    "def iterate_power_method(matrix_X, epsilon=1e-10):    \n",
    "    num_rows, num_columns = matrix_X.shape\n",
    "    start_vector = generate_random_unit_vector(num_columns)\n",
    "    previous_eigenvector = None\n",
    "    current_eigenvector = start_vector\n",
    "    covariance_matrix = np.dot(matrix_X.T, matrix_X)\n",
    "\n",
    "    # Power iteration until convergence\n",
    "    iteration_count = 0        \n",
    "    while True:\n",
    "        iteration_count += 1\n",
    "        previous_eigenvector = current_eigenvector\n",
    "        current_eigenvector = np.dot(covariance_matrix, previous_eigenvector)\n",
    "        current_eigenvector = current_eigenvector / norm(current_eigenvector)\n",
    "\n",
    "        if abs(np.dot(current_eigenvector, previous_eigenvector)) > 1 - epsilon:\n",
    "            return current_eigenvector\n",
    "\n",
    "def svd_decomposition(matrix_X, epsilon=1e-10):\n",
    "    num_rows, num_columns = matrix_X.shape\n",
    "    basis_change_list = []\n",
    "\n",
    "    for i in range(num_columns):\n",
    "        modified_matrix = matrix_X.copy()\n",
    "\n",
    "        for sigma, u, v in basis_change_list[:i]:\n",
    "            modified_matrix -= sigma * np.outer(u, v) \n",
    "\n",
    "        v_vector = iterate_power_method(modified_matrix, epsilon=epsilon)\n",
    "        u_sigma_vector = np.dot(matrix_X, v_vector)\n",
    "        sigma_value = norm(u_sigma_vector)  \n",
    "        u_vector = u_sigma_vector / sigma_value\n",
    "\n",
    "        basis_change_list.append((sigma_value, u_vector, v_vector))\n",
    "     \n",
    "    sigmas, us, v_transposes = [np.array(x) for x in zip(*basis_change_list)]\n",
    "\n",
    "    return sigmas, us.T, v_transposes\n",
    "\n",
    "\n",
    "dataset = np.random.random_sample((8, 3))\n",
    "results = svd_decomposition(dataset)\n",
    "print(\"sigmas\", results[0])\n",
    "print(\"u: data points in new coordinate system\", results[1])\n",
    "print(\"v transpose: change of basis matrix\", results[2]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d1325-520e-4ade-b411-a5d36a1ce74f",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition from Scikit-Learn (Using a Random Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "804099f0-5b32-41f0-b8ae-398570d3518f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmas [2.57223046 1.12178285 0.38969376]\n",
      "u: data points in new coordinate system [[-0.3150055   0.03347771  0.06858331 -0.20636181  0.00809513 -0.65078201\n",
      "  -0.14368477 -0.63885353]\n",
      " [-0.45305761 -0.08238565 -0.55510527 -0.09441373 -0.49168415  0.20584436\n",
      "  -0.42646603  0.06997943]\n",
      " [-0.32832461 -0.28893255 -0.44266749  0.02925347  0.14459012 -0.10789956\n",
      "   0.76054828  0.03046899]\n",
      " [-0.21235218 -0.02664549  0.07840387  0.9602729  -0.03933807 -0.08077358\n",
      "  -0.09324217 -0.09570481]\n",
      " [-0.39056276  0.42227756 -0.15691926 -0.02902804  0.74688276  0.16815048\n",
      "  -0.22033788  0.09496784]\n",
      " [-0.26469668 -0.53402117  0.36459409 -0.09209808  0.1494365   0.56750926\n",
      "  -0.03337409 -0.39728365]\n",
      " [-0.43199541  0.55382614  0.41868961 -0.07544635 -0.39105299  0.18644024\n",
      "   0.36996567  0.03326289]\n",
      " [-0.36523446 -0.37105189  0.39012728 -0.10249008  0.05137913 -0.3617221\n",
      "  -0.1543257   0.63946983]]\n",
      "v transpose: change of basis matrix [[-0.50399864 -0.4444467  -0.74057579]\n",
      " [-0.81511563  0.52829539  0.23767726]\n",
      " [ 0.28560791  0.72344392 -0.62853561]]\n"
     ]
    }
   ],
   "source": [
    "# Singular-value decomposition\n",
    "from numpy import array\n",
    "from scipy.linalg import svd\n",
    "\n",
    "results = svd(dataset)\n",
    "print(\"sigmas\", results[1])\n",
    "print(\"u: data points in new coordinate system\", results[0])\n",
    "print(\"v transpose: change of basis matrix\", results[2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9477f-7f4d-4f5b-99ad-35cd88fcde1c",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\r\n",
    "My manual singular value decomposition (SVD) implementation yields results similar to scikit-learn's SVD for several reasons. Firstly, both implementations follow the same algorithmic principles inherent to SVD, incorporating iterative methods, power iteration, and matrix factorization. The use of a random initialization technique, specifically power iteration with a random unit vector, is a shared approach in both implementations. Additionally, a convergence criterion based on the dot product of current and previous eigenvectors is employed by both to determine the termination of the power iteration process. Given the nature of numerical computations involved in SVD, both implementations may exhibit slight variations due to differences in numerical precision. Both implementations ensure the normalization of eigenvectors and singular vectors for consistency. Adjusting the signs of computed singular vectors is another common practice shared by both implementations. While small differences may exist due to variations in numerical techniques, edge case handling, and specific implementation details, they do not significantly impact the overall similarity in results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
